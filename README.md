# ğŸ§  MCP-Powered HR Assistant (FastAPI + LLM Tool Calling)

## ğŸ“Œ Overview

This project demonstrates how to build a context-aware AI assistant that interacts with structured enterprise data using the Model Context Protocol (MCP).

Instead of relying only on embeddings and RAG, the assistant dynamically discovers tools, executes database queries, and reasons over live data using an LLM.

The use case is a real-world HR / Employee Analytics system, supporting queries like:

- Who works in a specific location?
- What is an employeeâ€™s salary?
- Who reports to whom?
- Follow-up questions using conversational context

## â“ Why not just RAG?

Traditional RAG systems work well for unstructured text (docs, PDFs, policies), but they struggle with:

- Live databases
- Aggregations (highest salary, averages)
- Filters (location, department)
- Deterministic answers
- Follow-up questions using pronouns (â€œhisâ€, â€œthat employeeâ€)

MCP solves this by allowing the LLM to call tools instead of guessing.

## ğŸ§© What is MCP (Model Context Protocol)?

MCP is a protocol, not a library.

It defines:

- How tools are described
- How models discover tools
- How models call tools
- How results are returned and reasoned over

In this project:

- Each FastAPI endpoint = an MCP tool
- `/mcp/tools` = tool discovery
- The LLM decides when and which tool to call

## ğŸ—ï¸ Architecture

```
User
  â†“
LLM (reasoning + memory)
  â†“ decides tool
MCP Client
  â†“
MCP Server (FastAPI)
  â†“
SQLite Employee Database
  â†“
Structured JSON result
  â†“
LLM formats final answer
```

**Key Design Principles**

- LLM is stateless
- Application provides conversation memory
- Database access is deterministic
- No embeddings for structured data
- No hard-coded intent logic

## ğŸ“‚ Project Structure

```
mcp-employee-ai/
â”‚
â”œâ”€â”€ init_db.py          # Creates and seeds the employee database
â”œâ”€â”€ mcp_server.py       # MCP tool server (FastAPI)
â”œâ”€â”€ mcp_client.py       # Simple MCP client (tool discovery + execution)
â”œâ”€â”€ llm_app.py          # LLM + MCP orchestration with conversation memory
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md
```

## ğŸ§ª Example Capabilities

The assistant can answer:

- â€œWhich location James is working from?â€
- â€œWhat is his salary?â€ (context-aware)
- â€œWho else works in London?â€
- â€œWho reports to EMP-1002?â€
- â€œList employees working in Bangaloreâ€

The assistant:

- Remembers context
- Resolves pronouns
- Chooses tools automatically
- Avoids hallucination

## âš™ï¸ Setup Instructions (Step by Step)

1ï¸âƒ£ Create virtual environment (recommended)

```bash
python -m venv venv_for_mcp
source venv_for_mcp/bin/activate
```

2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

3ï¸âƒ£ Initialize the database

This creates a SQLite database with realistic employee data (Example).

```bash
python init_db.py
```

4ï¸âƒ£ Start the MCP server

```bash
uvicorn mcp_server:app --port 3333
Or 
uvicorn mcp_server:app --reload --port 3333
```

Verify:

- Open http://127.0.0.1:3333/docs
- You should see MCP tools listed

5ï¸âƒ£ Run the LLM application

Open a new terminal (same venv):

```bash
python llm_app.py
```

## ğŸ“„ Note on `mcp_client.py`

You may notice a file named `mcp_client.py` in this repository.

This file is not required for the main application flow and is not used by `llm_app.py`.

Why it exists:

- It is a standalone MCP client used to:
  - Test MCP tool discovery (`/mcp/tools`)
  - Execute MCP tools directly without involving an LLM
  - Debug database queries and MCP server behavior in isolation

Actual execution flow:

- `llm_app.py` acts as the real MCP client when the LLM is running
- `mcp_client.py` exists purely for learning, debugging, and demonstration purposes

You can safely remove this file if you only want the LLM-driven experience.

Optional one-liner (even shorter)

If you want something ultra-minimal:

`mcp_client.py` is an optional utility to interact with the MCP server without an LLM. The main application (`llm_app.py`) already acts as the MCP client during normal execution.


## ğŸ’¬ Sample Conversation

Ask a question: Which location James is working from?

â†’ James is working from the London location.

Ask a question: What is his salary?

â†’ James's base salary is 2,500,000.

Ask a question: Who else works in London?

â†’ Emily Clark, Oliver Wilson, Daniel Miller...

## ğŸ§  Key Learnings from This Project

- MCP is a protocol, not a framework
- LLMs do not have memory â€” applications must manage it
- Every tool call must be answered (tool_call_id)
- Structured data should be queried, not embedded
- Agent systems require strict orchestration

## ğŸš€ Why This Project Matters

This project demonstrates:

- Agentic LLM design
- Tool discovery and execution
- Context-aware conversations
- Enterprise-style data access
- Real debugging of protocol-level issues

It goes beyond basic RAG demos and reflects real-world GenAI system design.

## ğŸ”® Possible Improvements

- Add salary analytics tools (highest / average salary)
- Role-based access (HR vs Manager)
- Redis-backed conversation memory
- Combine MCP + RAG for policy documents
- Add caching for tool calls

## ğŸ‘¤ Author

Built by Ajmal
Sharing learnings around GenAI, MCP, and agentic systems.
